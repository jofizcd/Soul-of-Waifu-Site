<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title data-translate-key="pageTitle">All About LLMs - Documentation Soul of Waifu</title>
    <link rel="icon" type="image/x-icon" href="../../../assets/icons/logotype.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        :root {
            /* Dark Theme Colors */
            --dark-primary: #4361ee;
            --dark-primary-dark: #3a56d4;
            --dark-primary-light: #4868ff;
            --dark-secondary: #7209b7;
            --dark-accent: #4cc9f0;
            --dark-dark: #0f172a;
            --dark-darker: #0c1221;
            --dark-darkest: #080c16;
            --dark-light: #f1f5f9;
            --dark-gray: #94a3b8;
            --dark-card-bg: rgba(30, 41, 59, 0.7);
            --dark-card-border: rgba(71, 85, 105, 0.5);
            --dark-section-bg: rgba(15, 23, 42, 0.6);
            --dark-code-bg: #1e293b;
            --dark-hover: rgba(67, 97, 238, 0.15);

            /* Light Theme Colors */
            --light-primary: #4361ee;
            --light-primary-dark: #3a56d4;
            --light-primary-light: #4868ff;
            --light-secondary: #7209b7;
            --light-accent: #4cc9f0;
            --light-dark: #0f172a;
            --light-darker: #0c1221;
            --light-darkest: #080c16;
            --light-light: #f8fafc;
            --light-gray: #64748b;
            --light-card-bg: rgba(255, 255, 255, 0.9);
            --light-card-border: rgba(203, 213, 225, 0.8);
            --light-section-bg: rgba(241, 245, 249, 0.8);
            --light-code-bg: #f1f5f9;
            --light-hover: rgba(67, 97, 238, 0.1);

            /* Current Theme */
            --primary: var(--dark-primary);
            --primary-dark: var(--dark-primary-dark);
            --primary-light: var(--dark-primary-light);
            --secondary: var(--dark-secondary);
            --accent: var(--dark-accent);
            --dark: var(--dark-dark);
            --darker: var(--dark-darker);
            --darkest: var(--dark-darkest);
            --light: var(--dark-light);
            --gray: var(--dark-gray);
            --card-bg: var(--dark-card-bg);
            --card-border: var(--dark-card-border);
            --section-bg: var(--dark-section-bg);
            --code-bg: var(--dark-code-bg);
            --hover: var(--dark-hover);
        }
        body {
            background: linear-gradient(135deg, var(--darker), var(--darkest));
            color: var(--light);
            min-height: 100vh;
            overflow-x: hidden;
        }
        body.light-theme {
            --primary: var(--light-primary);
            --primary-dark: var(--light-primary-dark);
            --primary-light: var(--light-primary-light);
            --secondary: var(--light-secondary);
            --accent: var(--light-accent);
            --dark: var(--light-dark);
            --darker: var(--light-darker);
            --darkest: var(--light-darkest);
            --light: var(--light-dark);
            --gray: var(--light-gray);
            --card-bg: var(--light-card-bg);
            --card-border: var(--light-card-border);
            --section-bg: var(--light-section-bg);
            --code-bg: var(--light-code-bg);
            --hover: var(--light-hover);
            background: linear-gradient(135deg, #f1f5f9, #e2e8f0);
            color: var(--dark);
        }
        header {
            background: rgba(15, 23, 42, 0.95);
            backdrop-filter: blur(10px);
            padding: 1rem 2rem;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            border-bottom: 1px solid var(--card-border);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        }
        body.light-theme header {
            background: rgba(255, 255, 255, 0.95);
            border-bottom: 1px solid var(--card-border);
        }
        .header-container {
            max-width: 1400px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .logo {
            display: flex;
            align-items: center;
            gap: 1rem;
            padding-bottom: 10px;
        }
        .logo-icon {
            width: 40px;
            height: 40px;
            position: relative;
        }
        .logo a {
            display: flex;
            align-items: center;
            text-decoration: none;
        }
        .logo-text {
            font-size: 1.5rem;
            font-weight: 700;
            color: var(--light);
            text-decoration: none;
        }
        body.light-theme .logo-text {
            color: var(--dark);
        }
        .header-controls {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        .social-icons {
            display: flex;
            gap: 0.5rem;
        }
        .social-icon {
            width: 36px;
            height: 36px;
            border-radius: 50%;
            background: var(--card-bg);
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--light);
            text-decoration: none;
            transition: all 0.3s ease;
            border: 1px solid var(--card-border);
        }
        body.light-theme .social-icon {
            background: rgba(255, 255, 255, 0.8);
            color: var(--dark);
        }
        .social-icon:hover {
            background: var(--primary);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }
        .theme-language-controls {
            display: flex;
            gap: 1rem;
            align-items: center;
        }
        .theme-toggle {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 20px;
            width: 50px;
            height: 25px;
            position: relative;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        .theme-toggle-slider {
            position: absolute;
            top: 1px;
            left: 1px;
            width: 21px;
            height: 21px;
            background: var(--primary-light);
            border-radius: 50%;
            transition: all 0.3s ease;
        }
        body.light-theme .theme-toggle-slider {
            transform: translateX(25px);
        }
        .language-selector {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 20px;
            padding: 0.3rem 0.8rem;
            color: var(--light);
            font-weight: 500;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        body.light-theme .language-selector {
            color: var(--dark);
        }
        .doc-container {
            display: flex;
            min-height: 100vh;
            padding-top: 78px;
        }
        .doc-sidebar {
            width: 280px;
            background: var(--card-bg);
            border-right: 1px solid var(--card-border);
            padding: 2rem 1rem;
            position: fixed;
            height: calc(100vh - 76px);
            overflow-y: auto;
            transition: all 0.3s ease;
            scrollbar-width: thin;
            scrollbar-color: var(--primary-light) var(--card-bg);
        }
        .doc-sidebar::-webkit-scrollbar {
            width: 8px;
        }
        .doc-sidebar::-webkit-scrollbar-track {
            background: var(--card-bg);
            border-radius: 4px;
        }
        .doc-sidebar::-webkit-scrollbar-thumb {
            background-color: var(--primary-light);
            border-radius: 4px;
            border: 2px solid var(--card-bg);
        }
        .doc-sidebar h3 {
            color: var(--primary-light);
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--card-border);
            font-size: 1.1rem;
        }
        .doc-sidebar ul {
            list-style: none;
            margin-bottom: 2rem;
        }
        .doc-sidebar ul li {
            margin-bottom: 0.3rem;
        }
        .doc-sidebar a {
            color: #cbd5e1;
            text-decoration: none;
            display: block;
            padding: 0.6rem 1rem;
            border-radius: 6px;
            transition: all 0.2s ease;
            font-size: 0.95rem;
            position: relative;
            overflow: hidden;
        }
        body.light-theme .doc-sidebar a {
            color: #4a5568;
        }
        .doc-sidebar a:hover {
            background: var(--hover);
            color: var(--primary-light);
        }
        .doc-sidebar a.active {
            background: var(--primary);
            color: white !important;
        }
        body.light-theme .doc-sidebar a.active {
            color: white !important;
        }
        .doc-main {
            flex: 1;
            margin-left: 280px;
            padding: 2rem;
        }
        .doc-content {
            max-width: 900px;
            margin: 0 auto;
        }
        .doc-content h1 {
            font-size: 2.5rem;
            margin-bottom: 1.5rem;
            color: white;
            padding-bottom: 1rem;
            border-bottom: 1px solid var(--card-border);
            scroll-margin-top: 100px;
        }
        body.light-theme .doc-content h1 {
            color: var(--dark);
        }
        .doc-content h2 {
            font-size: 2rem;
            margin: 2rem 0 1rem;
            color: white;
            scroll-margin-top: 100px;
            position: relative;
            padding-left: 1.5rem;
        }
        body.light-theme .doc-content h2 {
            color: var(--dark);
        }
        .doc-content h2:before {
            content: "";
            position: absolute;
            left: 0;
            top: 0.5rem;
            width: 4px;
            height: 1.5rem;
            background: var(--primary-light);
            border-radius: 2px;
        }
        .doc-content h3 {
            font-size: 1.5rem;
            margin: 1.5rem 0 1rem;
            color: white;
            scroll-margin-top: 100px;
        }
        body.light-theme .doc-content h3 {
            color: var(--dark);
        }
        .doc-content p {
            font-size: 1.1rem;
            line-height: 1.7;
            margin-bottom: 1.5rem;
            color: #cbd5e1;
        }
        body.light-theme .doc-content p {
            color: #4a5568;
        }
        .doc-content ul, 
        .doc-content ol {
            margin: 1rem 0 1.5rem 2rem;
        }
        .doc-content li {
            margin-bottom: 0.5rem;
            line-height: 1.6;
            color: #cbd5e1;
        }
        body.light-theme .doc-content li {
            color: #4a5568;
        }
        .doc-content code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: var(--accent);
            font-size: 0.95rem;
        }
        .doc-content pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            border: 1px solid var(--card-border);
            position: relative;
        }
        body.light-theme .doc-content pre {
            background: #f1f5f9;
        }
        .doc-content pre code {
            background: none;
            padding: 0;
            color: #cbd5e1;
        }
        body.light-theme .doc-content pre code {
            color: #4a5568;
        }
        .doc-content .alert {
            padding: 1rem;
            border-radius: 8px;
            margin: 1.5rem 0;
            border-left: 4px solid var(--primary-light);
            background: rgba(67, 97, 238, 0.1);
            position: relative;
            padding-left: 3rem;
        }
        body.light-theme .doc-content .alert {
            background: rgba(67, 97, 238, 0.05);
        }
        .doc-content .alert:before {
            content: "!";
            position: absolute;
            left: 1rem;
            top: 50%;
            transform: translateY(-50%);
            width: 24px;
            height: 24px;
            background: var(--primary-light);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }
        .doc-content .alert-warning {
            border-left-color: #f59e0b;
            background: rgba(245, 158, 11, 0.1);
        }
        body.light-theme .doc-content .alert-warning {
            background: rgba(245, 158, 11, 0.05);
        }
        .doc-content .alert-warning:before {
            background: #f59e0b;
        }
        .doc-content .alert-info {
            border-left-color: #3b82f6;
            background: rgba(59, 130, 246, 0.1);
        }
        body.light-theme .doc-content .alert-info {
            background: rgba(59, 130, 246, 0.05);
        }
        .doc-content .alert-info:before {
            background: #3b82f6;
        }
        .model-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 8px;
            overflow: hidden;
        }
        .model-table th,
        .model-table td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--card-border);
        }
        .model-table th {
            background: rgba(67, 97, 238, 0.15);
            color: var(--primary-light);
            font-weight: 600;
        }
        .model-table tr:last-child td {
            border-bottom: none;
        }
        body.light-theme .model-table th {
            background: rgba(67, 97, 238, 0.1);
        }
        .screenshot-container {
            text-align: center;
            margin: 1.5rem 0;
        }
        .screenshot-container img {
            max-width: 100%;
            height: auto;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
            border: 1px solid var(--card-border);
        }
        .toc {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 10px;
            padding: 1.5rem;
            margin: 2rem 0;
        }
        .toc h3 {
            color: var(--primary-light);
            margin-bottom: 1rem;
            margin-top: 0.5rem;
        }
        .toc ul {
            list-style: none;
            margin: 0;
        }
        .toc ul li {
            margin-bottom: 0.5rem;
        }
        .toc ul li a {
            color: var(--primary-light);
            text-decoration: none;
            display: flex;
            align-items: center;
        }
        .toc ul li a:before {
            content: "→";
            margin-right: 0.5rem;
        }
        .pagination {
            display: flex;
            justify-content: space-between;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--card-border);
        }
        .pagination-link {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--primary-light);
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 6px;
            transition: all 0.2s ease;
            background: var(--card-bg);
            border: 1px solid var(--card-border);
        }
        .pagination-link:hover {
            background: var(--hover);
        }
        .pagination-next {
            margin-left: auto;
        }
        @media (max-width: 1200px) {
            .doc-sidebar {
                width: 240px;
            }
            .doc-main {
                margin-left: 240px;
            }
        }
        @media (max-width: 992px) {
            .doc-container {
                flex-direction: column;
            }
            .doc-sidebar {
                width: 100%;
                position: relative;
                height: auto;
                border-right: none;
                border-bottom: 1px solid var(--card-border);
            }
            .doc-main {
                margin-left: 0;
            }
        }
        @media (max-width: 768px) {
            header {
                padding: 1rem;
            }
            .header-container {
                flex-direction: column;
                gap: 1rem;
            }
            .logo {
                align-self: flex-start;
            }
            .header-controls {
                width: 100%;
                flex-wrap: wrap;
            }
            .social-icons {
                margin-left: auto;
            }
            .theme-language-controls {
                margin-left: auto;
            }
            .doc-content h1 {
                font-size: 2rem;
            }
            .doc-content h2 {
                font-size: 1.7rem;
            }
            .doc-content h3 {
                font-size: 1.4rem;
            }
        }
        @media (max-width: 480px) {
            .social-icon span {
                display: none;
            }
            .social-icon {
                width: 32px;
                height: 32px;
            }
            .doc-content h1 {
                font-size: 1.8rem;
            }
            .doc-content h2 {
                font-size: 1.5rem;
            }
            .doc-content h3 {
                font-size: 1.3rem;
            }
            .doc-content p {
                font-size: 1rem;
            }
        }
        footer {
            background: var(--darkest);
            padding: 1rem;
            border-top: 1px solid var(--card-border);
            text-align: center;
        }
        body.light-theme footer {
            background: #f8fafc;
            border-top: 1px solid var(--card-border);
        }
        .footer-content {
            margin: auto;
            text-align: right;
            color: #94a3b8;
        }
        body.light-theme .footer-content {
            color: #718096;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .doc-content > * {
            animation: fadeIn 0.5s ease-out;
        }
        .screenshots-row {
            display: flex;
            gap: 1rem;
            margin: 1.5rem 0;
        }
        .screenshots-row .screenshot-container {
            flex: 1;
            margin: 0;
        }
        @media (max-width: 768px) {
            .screenshots-row {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="header-container">
            <div class="logo">
                <a href="../../../index.html">
                    <div class="logo-icon">
                        <img src="../../../assets/icons/logotitle.png" alt="logotype" width="160px" height="50px" class="logo-dark">
                        <img src="../../../assets/icons/logotitle_black.png" alt="logotype" width="160px" height="50px" class="logo-light" style="display: none;">
                    </div>
                </a>
            </div>
            <div class="header-controls">
                <div class="social-icons">
                    <a href="https://github.com/jofizcd/Soul-of-Waifu" class="social-icon" title="GitHub" data-translate-title="githubTitle">
                        <i class="fab fa-github"></i>
                    </a>
                    <a href="https://discord.com/invite/6vFtQGVfxM" class="social-icon" title="Discord" data-translate-title="discordTitle">
                        <i class="fab fa-discord"></i>
                    </a>
                    <a href="https://www.donationalerts.com/r/jofizcd" class="social-icon" title="Donation Alerts">
                        <i class="fa-solid fa-hand-holding-dollar"></i>
                    </a>
                </div>
                <div class="theme-language-controls">
                    <div class="theme-toggle" id="themeToggle">
                        <div class="theme-toggle-slider"></div>
                    </div>
                    <select class="language-selector" id="languageSelector">
                        <option value="ru">RU</option>
                        <option value="en">EN</option>
                    </select>
                </div>
            </div>
        </div>
    </header>
    <div class="doc-container">
        <aside class="doc-sidebar">
            <h3 class="sidebar-title" data-translate-key="sidebarTitle">Содержание</h3>
            <ul>
                <li><a href="../../overview/overview.html" class="sidebar-introduction" data-translate-key="sidebarIntroduction">Введение</a></li>
                <li><a href="../../installation/installation.html" class="sidebar-installation" data-translate-key="sidebarInstallation">Установка</a></li>
                <li><a href="../../quick-start/quickstart.html" class="sidebar-quick-start" data-translate-key="sidebarQuickStart">Быстрый старт</a></li>
                <li><a href="../../character-creation/character-creation.html" class="sidebar-character-creation" data-translate-key="sidebarCharacterCreation">Создание персонажей</a></li>
                <li><a href="../../llm-setup/llm-setup.html" class="sidebar-llm-setup" data-translate-key="sidebarLlmSetup">Настройка моделей</a></li>
                <li><a href="../../tts/tts.html" class="sidebar-tts-stt" data-translate-key="sidebarTtsStt">Озвучка</a></li>
                <li><a href="../../avatars/avatars.html" class="sidebar-avatars" data-translate-key="sidebarAvatars">Аватары персонажей</a></li>
            </ul>
            <h3 class="sidebar-course-title" data-translate-key="sidebarCourseTitle">Вводный курс в AI RP</h3>
            <ul>
                <li><a href="llm.html" class="sidebar-course-llm active" data-translate-key="sidebarCourseLlm">Всё про LLM</a></li>
                <li><a href="../../course/persona/persona.html" class="sidebar-course-persona" data-translate-key="sidebarCoursePersona">Персона</a></li>
                <li><a href="../../course/system-prompt/system-prompt.html" class="sidebar-course-system-prompt" data-translate-key="sidebarCourseSystemPrompt">Системный промпт</a></li>
                <li><a href="../../course/lorebooks/lorebooks.html" class="sidebar-course-lorebooks" data-translate-key="sidebarCourseLorebooks">Лорбуки</a></li>
                <li><a href="../../course/smart-memory/smart-memory.html" class="sidebar-course-smartmemory" data-translate-key="sidebarCourseSmartMemory">Умная Память</a></li>
                <li><a href="../../course/sow-system/sow-system.html" class="sidebar-course-soul-system" data-translate-key="sidebarCourseSoulSystem">Soul of Waifu System</a></li>
            </ul>
            <h3 class="sidebar-development-title" data-translate-key="sidebarDevelopmentTitle">Развитие программы</h3>
            <ul>
                <li><a href="../../program-support/community/community.html" class="sidebar-community" data-translate-key="sidebarCommunity">Сообщество</a></li>
                <li><a href="../../program-support/support/support.html" class="sidebar-support" data-translate-key="sidebarSupport">Поддержать автора</a></li>
            </ul>
        </aside>
        <main class="doc-main">
            <div class="doc-content">
                <h1 id="llm-overview" class="doc-title" data-translate-key="docTitle">Всё про LLM</h1>
                <p class="doc-intro" data-translate-key="docIntro">Всем привет, сейчас мы с вами будем учить некоторые вещи про большие и не очень языковые модели, после которых вы будете знать их предысторию, а также почему технология Transformer - это то, чему мы должны поклоняться до тех пор, пока не выйдет нечто более совершенное.</p>
                
                <div class="toc">
                    <h3 data-translate-key="tocTitle">Содержание раздела</h3>
                    <ul>
                        <li><a href="#what-are-llms" data-translate-key="tocWhatAreLlms">Что такое языковые модели и их история</a></li>
                        <li><a href="#transformer-architecture" data-translate-key="tocTransformerArchitecture">Архитектура Transformer</a></li>
                        <li><a href="#model-parameters" data-translate-key="tocModelParameters">Параметры моделей</a></li>
                        <li><a href="#quantization" data-translate-key="tocQuantization">Квантование</a></li>
                        <li><a href="#tokenization" data-translate-key="tocTokenization">Токенизация</a></li>
                        <li><a href="#context" data-translate-key="tocContext">Контекст</a></li>
                        <li><a href="#flash-attention" data-translate-key="tocFlashAttention">Flash Attention</a></li>
                        <li><a href="#gguf-format" data-translate-key="tocGgufFormat">Формат GGUF</a></li>
                        <li><a href="#llama-cpp" data-translate-key="tocLlamaCpp">Llama.CPP</a></li>
                    </ul>
                </div>
                
                <h2 id="what-are-llms" data-translate-key="whatAreLlmsTitle">Что такое языковые модели и их история</h2>
                
                <p data-translate-key="whatAreLlmsText1">Языковые модели (Large Language Models, LLM) представляют собой искусственные нейронные сети, обученные на огромных объемах текстовых данных для распознавания сложных паттернов, контекстных отношений и лингвистических нюансов в человеческом языке. Эти модели способны предсказывать вероятность следующего токена (слова или его части) в последовательности на основе предыдущего контекста.</p>
                
                <p data-translate-key="whatAreLlmsText2">История языковых моделей началась задолго до современных LLM. Еще в 1977 году был впервые применен алгоритм beam search для распознавания речи, который до сих пор используется как окончательный слой принятия решений в NLP-системах. Первоначально языковые модели были относительно простыми n-граммными моделями, где вероятность каждого слова зависела только от n-1 предыдущих слов. Однако с развитием технологий и увеличением вычислительных мощностей появились более сложные архитектуры.</p>
                
                <p data-translate-key="whatAreLlmsText3">Современные LLM, такие как GPT, Llama, Mistral и другие, представляют собой гигантские нейронные сети с миллиардами параметров. Обучение LLM требует колоссальных вычислительных ресурсов. Для обучения крупномасштабных моделей, таких как Llama или GPT, часто необходимо использовать десятки тысяч процессорных единиц. Большинство LLM обучаются на интернет-данных, включая Википедию, книги и другие текстовые источники, преобразованные в формат простого текста.</p>
                
                <h2 id="transformer-architecture" data-translate-key="transformerArchitectureTitle">Архитектура Transformer</h2>
                
                <p data-translate-key="transformerArchitectureText1">Современные LLM, как правило, используют decoder-only Transformer архитектуру и механизмы self-attention. Transformer, представленный в 2017 году в статье "Attention is All You Need", кардинально изменил подход к обработке естественного языка, заменив рекуррентные и сверточные нейронные сети на механизмы внимания.</p>
                
                <div class="screenshot-container">
                    <img src="../../../assets/screenshots/coursellm_1.png" alt="Статья Attention Is All You Need" data-translate-alt="attentionPaperScreenshotAlt">
                    <p class="screenshot-caption" data-translate-key="attentionPaperCaption" style="margin-top: 12px; font-size: 0.95em; color: #666;">Статья "Attention Is All You Need"</p>
                </div>
                
                <p data-translate-key="transformerArchitectureText2">Процесс работы LLM можно описать следующим образом:</p>
                
                <ol>
                    <li data-translate-key="transformerStep1">Токенизация: Входной текст разбивается на токены с помощью токенизатора. Токены могут представлять собой целые слова, части слов или даже отдельные символы, в зависимости от используемого алгоритма токенизации.</li>
                    <li data-translate-key="transformerStep2">Эмбеддинги: Каждому токену присваивается числовой вектор, который кодирует его семантическое значение. Этот вектор является отправной точкой для понимания модели. В некоторых архитектурах векторы также содержат информацию о позиции токена в последовательности, в других — эта информация добавляется позже.</li>
                    <li data-translate-key="transformerStep3">Decoder Layers: Токены проходят через серию идентичных декодерных слоев (количество может варьироваться от 24 до 80+ в зависимости от размера модели).</li>
                    <li data-translate-key="transformerStep4">Output Generation: После прохождения всех слоев модель использует функцию softmax для преобразования оценок, указывающих, насколько модель считает каждый токен вероятным, в вероятности. Эти вероятности показывают, насколько вероятен каждый токен как следующее слово в выходной последовательности.</li>
                </ol>
                
                <h2 id="model-parameters" data-translate-key="modelParametersTitle">Параметры моделей</h2>
                
                <p data-translate-key="modelParametersText1">Параметры языковой модели - это численные значения, определяющие поведение нейронной сети. Чем больше параметров имеет модель, тем сложнее зависимости она может улавливать, однако для её запуска потребуется больше ресурсов. Размер модели обычно выражается в миллиардах параметров (7B, 13B, 70B и т.д.). Для оценки требований к памяти можно использовать простое правило: модель в формате FP16 занимает примерно 2 гигабайта памяти на каждый миллиард параметров. Например:</p>
                
                <ul>
                    <li data-translate-key="modelSizeExample1">Модель 7B в FP16 занимает около 14 ГБ</li>
                    <li data-translate-key="modelSizeExample2">Модель 13B в FP16 занимает около 26 ГБ</li>
                    <li data-translate-key="modelSizeExample3">Модель 70B в FP16 занимает около 140 ГБ</li>
                </ul>
                
                <p data-translate-key="modelParametersText2">Однако для фактического запуска модели требуется немного больше памяти, чем размер самой модели, поскольку необходимо хранить данные текущих вычислений и обработанный контекст. Например, теоретический запуск модели 7B потребует минимум 16 ГБ видеопамяти (в реальности почти 20 ГБ). Это очень много для такого количества параметров, и мы можем оптимизировать вес модели благодаря такой технологии как квантование.</p>
                
                <h2 id="quantization" data-translate-key="quantizationTitle">Квантование</h2>
                
                <p data-translate-key="quantizationText1">Квантование - это процесс сжатия языковой модели, который позволяет снизить точность представления весов модели с 16 бит до 8, 4, 3 или даже 2 бит, сохраняя при этом основной функционал и качество генерации. Это критически важно для запуска моделей на обычных домашних компьютерах с ограниченными ресурсами видеопамяти.</p>
                
                <ul>
                    <li data-translate-key="quantizationBenefit1">Результаты при генерации токенов моделью, квантованной в 8 бит, практически не имеют отличий от 16-битной версии</li>
                    <li data-translate-key="quantizationBenefit2">Большие модели (70B) лучше переносят квантование даже в малые битности (~3 бита), тогда как на малых моделях (7B) заметная деградация может проявиться уже при 4 битах</li>
                    <li data-translate-key="quantizationBenefit3">Квантованные (особенно в малую битность) модели малопригодны для обучения</li>
                    <li data-translate-key="quantizationBenefit4">Сильное ужатие (ниже 3 бит) приведет к поломке и деградации модели, вы это сразу почувствуете.</li>
                </ul>
                
                <p data-translate-key="quantizationText2">Для понимания квантования необходимо знать структуру языковой модели. Каждый слой модели состоит из двух основных типов тензоров:</p>
                
                <ol>
                    <li data-translate-key="tensorType1">Тензоры внимания (attn) - отвечают за механизм self-attention, определяющий, насколько качественной будет квантованная модель</li>
                    <li data-translate-key="tensorType2">Тензоры полносвязной сети (ffn) - отвечают за непосредственную обработку информации после механизма внимания</li>
                </ol>
                
                <p data-translate-key="quantizationText3">В формате GGUF квантование обозначается как Qx_K_S/M/L, где:</p>
                
                <ul>
                    <li data-translate-key="ggufFormat1">Qx - основной уровень квантования для тензоров ffn (полносвязной сети), где x — число от 1 до 8</li>
                    <li data-translate-key="ggufFormat2">K - указывает на версию формата квантования</li>
                    <li data-translate-key="ggufFormat3">S/M/L - определяет, насколько выше будут квантованы важные тензоры внимания (attn)</li>
                </ul>
                
                <ul>
                    <li data-translate-key="ggufLevels1">S (Small) - важные тензоры внимания квантуются на 1 шаг выше (Qx+1)</li>
                    <li data-translate-key="ggufLevels2">M (Medium) - важные тензоры внимания квантуются на 2 шага выше (Qx+2)</li>
                    <li data-translate-key="ggufLevels3">L (Large) - важные тензоры внимания квантуются на 3 шага выше (Qx+3)</li>
                </ul>
                
                <p data-translate-key="quantizationExample">К примеру, в Q4_K_M:</p>
                
                <ul>
                    <li data-translate-key="q4kmExample1">Основной уровень квантования — 4 бита (Q4)</li>
                    <li data-translate-key="q4kmExample2">Важные тензоры внимания квантуются на 2 шага выше — до 6 бит (Q6)</li>
                </ul>
                
                <h3 data-translate-key="quantizationTypesTitle">Типы квантования и их особенности:</h3>
                
                <h4 data-translate-key="staticQuantizationTitle">Статическое квантование (Qx_K_S/M/L)</h4>
                
                <p data-translate-key="staticQuantizationText">Это наиболее распространенный тип квантования. Важно понимать, что разные создатели квантов могут иметь разные интерпретации "важных тензоров". Например, некоторые квантуют ffn_up и ffn_gate на 1 шаг ниже, чем основной уровень, считая это оптимальным. Это приводит к ситуации, когда кванты с одинаковым названием (например, Q4_K_M) от разных создателей могут вести себя по-разному.</p>
                
                <h4 data-translate-key="iqQuantizationTitle">IQ кванты</h4>
                
                <p data-translate-key="iqQuantizationText1">IQ кванты используют матрицу важности (imatrix), которая создается на основе типичного использования модели. Процесс создания imatrix включает сбор типичных примеров использования модели (тексты, программирование, факты и т.д.) и анализ, какие части модели активируются при их обработке.</p>
                
                <p data-translate-key="iqQuantizationText2">Преимущество IQ квантования:</p>
                
                <ul>
                    <li data-translate-key="iqBenefit1">Уменьшение размера модели при сохранении качества</li>
                    <li data-translate-key="iqBenefit2">Более эффективное распределение битности между тензорами</li>
                </ul>
                
                <h4 data-translate-key="dynamicQuantizationTitle">Динамическое квантование</h4>
                
                <p data-translate-key="dynamicQuantizationText1"><strong>UD (Unsloth Dynamic 2.0)</strong></p>
                
                <p data-translate-key="udQuantizationText">Это передовая технология квантования, которая динамически определяет, какие части модели требуют более высокой точности. Например, UD-Q2_K_XL (2-битное квантование) может давать качество, близкое к оригиналу, теряя всего несколько процентов на примере DeepSeek R1, будучи в 3.3 раза меньше по размеру (212 ГБ против 700 ГБ).</p>
                
                <p data-translate-key="dynamicQuantizationText2"><strong>R4</strong></p>
                
                <p data-translate-key="r4QuantizationText">R4 - это state-of-the-art квантование от ikawrakow, позволяющее создать самый маленький квант для крупных моделей. Например, R4 позволяет создать квант DeepSeek R1 размером всего 130 ГБ, который может работать на домашних ПК с 128 ГБ RAM и одной GPU.</p>
                
                <p data-translate-key="dynamicQuantizationText3"><strong>iqN_rt (Trellis квантование)</strong></p>
                
                <p data-translate-key="trellisQuantizationText">Это новейший тип квантования, аналогичный QTIP, используемому в exl3. Trellis квантование находит оптимальные параметры для каждого блока, позволяет лучше сохранить свойства оригинальной модели при минимальном количестве бит на вес (bpw).</p>
                
                <h2 id="tokenization" data-translate-key="tokenizationTitle">Токенизация</h2>
                
                <p data-translate-key="tokenizationText1">Токенизация - это фундаментальный процесс, который включает разделение заданного текста на отдельные единицы, называемые токенами. Токены могут представлять собой целые слова, части слов или даже отдельные символы, в зависимости от уровня детализации, необходимого для конкретной задачи. Причина необходимости токенизации достаточно проста: машины не понимают слов в их естественной форме, поэтому необходимо преобразовать слова в численные представления. Однако разделение текста по алфавитным буквам слишком неэффективно (потребуется в среднем 4 токена на одно слово), а разделение по словам тоже неоптимально (потребуется словарь размером около 171 476 слов только для английского языка). Поэтому создается небольшая модель токенизатора, обученная на большом объеме текста, чтобы обеспечить наиболее эффективную токенизацию с минимальным размером словаря. Например, токенизатор BPE (Byte Pair Encoding) или его варианты, используемые в современных моделях, создают словарь фиксированного размера (обычно 32 000-50 000 токенов), который может эффективно кодировать как целые слова, так и их части.</p>
                
                <p data-translate-key="tokenizationText2">Важно отметить, что разные языки имеют разную эффективность токенизации. Английский язык (и латиница в целом) потребляет наименьшее количество токенов (в среднем 1-2 на слово), тогда как кириллица может потребовать вдвое больше токенов на тот же объем символов. Эмодзи и специальные символы могут потребовать нескольких токенов на один символ.</p>
                
                <h2 id="context" data-translate-key="contextTitle">Контекст</h2>
                
                <p data-translate-key="contextText1">Контекст - это объем текста, который модель обрабатывает за один раз. Он включает в себя историю чата, системный промт, карточки персонажей, авторские заметки, лорбуки и другие элементы, которые формируют основу для генерации ответа. Модель "помнит" и учитывает только то, что находится в пределах контекста. Базовый контекст в новых моделях составляет 4096 и более токенов, в более ранних моделях было 2048 или меньше. Это означает, что если ваши сообщения в чате, упоминание важного события или определение какого-то термина выходит за пределы этого контекста, модель перестанет помнить об этом и будет интерпретировать запрос на основе своих общих знаний.</p>
                
                <p data-translate-key="contextText2">Проблема ограниченного контекста решается несколькими способами:</p>
                
                <ul>
                    <li data-translate-key="contextSolution1">Суммаризацией (созданием краткого содержания предыдущего контекста)</li>
                    <li data-translate-key="contextSolution2">Использованием лорбуков (специальных хранилищ знаний с информацией о мире и персонажах)</li>
                    <li data-translate-key="contextSolution3">Векторными базами данных для поиска релевантной информации</li>
                    <li data-translate-key="contextSolution4">Простым увеличением контекста</li>
                </ul>
                
                <p data-translate-key="contextText3">Контекст можно увеличить через настройки LLM в Soul of Waifu, однако не забудьте перезагрузить локальную языковую модель, если на момент изменения значения контекстного окна она была запущена.</p>
                
                <p data-translate-key="contextText4">Сильное увеличение контекста (более чем в 2-3 раза относительно исходного для модели) может привести к ухудшению качества ответов. Некоторые модели изначально обучаются с использованием подобных параметров, что позволяет увеличивать контекст до огромных величин (32K, 64K, 100K и более) без существенного ухудшения качества.</p>
                
                <h2 id="flash-attention" data-translate-key="flashAttentionTitle">Flash Attention</h2>
                
                <p data-translate-key="flashAttentionText1">Flash Attention - это оптимизация, разработанная для ускорения и снижения потребления памяти механизмов внимания в трансформерах. Эта технология особенно важна при работе с длинными контекстами, так как вычислительная сложность стандартного механизма внимания растет квадратично с увеличением длины последовательности. Стандартный механизм внимания требует O(n²) памяти и вычислений для последовательности длиной n, что становится узким местом при работе с длинными контекстами (8K, 32K, 128K токенов). Flash Attention решает эту проблему, оптимизируя вычисления и уменьшая потребление памяти. Включение Flash Attention экономит 20-30% VRAM для длинных контекстов и особенно эффективно при использовании llama.cpp, которая как раз и используется в нашей программе. Эта оптимизация уменьшает влияние контекстного окна на память, причем эффект более заметен при большем количестве контекста. В сочетании с квантованием контекста до 4-бит можно дополнительно снизить нагрузку на память. Flash Attention работает за счет более эффективной организации вычислений и использования специализированных ядер для GPU, что позволяет уменьшить количество обращений к памяти и оптимизировать использование вычислительных ресурсов.</p>
                
                <h2 id="gguf-format" data-translate-key="ggufFormatTitle">Формат GGUF</h2>
                
                <p data-translate-key="ggufFormatText1">GGUF (GPT-Generated Unified Format) - это формат файлов для хранения квантованных языковых моделей, разработанный специально для использования с llama.cpp. Этот формат заменяет более старый формат GGML и предоставляет ряд улучшений и дополнительных возможностей:</p>
                
                <ul>
                    <li data-translate-key="ggufFeature1">Все данные хранятся в одном файле (в отличие от других форматов, где модель распространяется папкой с множеством файлов)</li>
                    <li data-translate-key="ggufFeature2">Поддерживает различные уровни квантования (от 2.5 до 8 бит)</li>
                    <li data-translate-key="ggufFeature3">Включает метаданные, такие как архитектура модели, токенизатор</li>
                    <li data-translate-key="ggufFeature4">Позволяет запускать модели на CPU, GPU или гибридно</li>
                    <li data-translate-key="ggufFeature5">Поддерживает матрицу важности (Imatrix) для более эффективного квантования</li>
                </ul>
                
                <p data-translate-key="ggufFormatText2">Имена файлов GGUF следуют определенному шаблону: "имя_модели-битность_формат.расширение". Например, "gemma-3-9b-q4_K_M.gguf".</p>
                
                <h2 id="llama-cpp" data-translate-key="llamaCppTitle">Llama.CPP</h2>
                
                <p data-translate-key="llamaCppText1">llama.cpp - это один из самых популярных программ для запуска языковых моделей, разработанный Georgi Gerganov. Этот проект представляет собой высокопроизводительную реализацию языковых моделей на языке C/C++, оптимизированную для запуска на различных устройствах, включая CPU, GPU и даже мобильные платформы.</p>
                
                <p data-translate-key="llamaCppText2">Основные особенности llama.cpp:</p>
                
                <ol>
                    <li data-translate-key="llamaCppFeature1">Кроссплатформенность: Работает на Windows, Linux, macOS, включая поддержку Apple Silicon через Metal.</li>
                    <li data-translate-key="llamaCppFeature2">Гибкость в использовании ресурсов: Позволяет запускать модели как на CPU, так и на GPU, а также в гибридном режиме, где часть слоев обрабатывается на GPU, а часть — на CPU.</li>
                    <li data-translate-key="llamaCppFeature3">Поддержка GGUF: Является основным движком для работы с моделями в формате GGUF.</li>
                    <li data-translate-key="llamaCppFeature4">Offloading слоев: Одна из ключевых функций - возможность выбора количества слоев, которые будут загружены в видеопамять (VRAM). Это реализуется через настройку GPU-слоев в программе. Например, выставленное значение в 30 слоёв загрузит первые 30 слоев модели в VRAM, остальные будут обрабатываться вашим процессором.</li>
                    <li data-translate-key="llamaCppFeature5">Поддержка различных оптимизаций: Например, Flash Attention для уменьшения потребления памяти при длинных контекстах.</li>
                    <li data-translate-key="llamaCppFeature6">Производительность: Даже при запуске на CPU llama.cpp демонстрирует впечатляющую производительность благодаря оптимизации для современных архитектур (AVX2, AVX-512).</li>
                </ol>
                
                <p data-translate-key="llamaCppText3">И как раз Soul of Waifu использует llama.cpp для запуска локальных языковых моделей внутри себя. Надеюсь, вы узнали много нового о языковых моделях, а в следующем разделе мы поговорим о персонах - инструменте, которые позволят вам представить себя персонажу.</p>
                
                <div class="pagination">
                    <a href="../../avatars/avatars.html" class="pagination-link pagination-prev">
                        <i class="fas fa-arrow-left"></i> <span class="pagination-prev-text" data-translate-key="paginationPrev">Аватары персонажей</span>
                    </a>
                    <a href="../persona/persona.html" class="pagination-link pagination-next">
                        <span class="pagination-next-text" data-translate-key="paginationNext">Персона</span> <i class="fas fa-arrow-right"></i>
                    </a>
                </div>
            </div>
        </main>
    </div>
    <footer>
        <div class="footer-content">
            <p class="copyright-text" data-translate-key="copyrightText">&copy; 2025 Soul of Waifu. Разработал jofizcd.</p>
        </div>
    </footer>
    <script>
        const translations = {
            ru: {
                pageTitle: "Всё про LLM - Документация Soul of Waifu",
                githubTitle: "GitHub",
                discordTitle: "Discord",
                sidebarTitle: "Содержание",
                sidebarIntroduction: "Введение",
                sidebarInstallation: "Установка",
                sidebarQuickStart: "Быстрый старт",
                sidebarCharacterCreation: "Создание персонажей",
                sidebarLlmSetup: "Настройка моделей",
                sidebarTtsStt: "Озвучка",
                sidebarAvatars: "Аватары персонажей",
                sidebarCourseTitle: "Вводный курс в AI RP",
                sidebarCourseLlm: "Всё про LLM",
                sidebarCoursePersona: "Персона",
                sidebarCourseSystemPrompt: "Системный промпт",
                sidebarCourseLorebooks: "Лорбуки",
                sidebarCourseSmartMemory: "Умная Память",
                sidebarCourseSoulSystem: "Soul of Waifu System",
                sidebarDevelopmentTitle: "Развитие программы",
                sidebarCommunity: "Сообщество",
                sidebarSupport: "Поддержать автора",
                docTitle: "Всё про LLM",
                docIntro: "Всем привет, сейчас мы с вами будем учить некоторые вещи про большие и не очень языковые модели, после которых вы будете знать их предысторию, а также почему технология Transformer - это то, чему мы должны поклоняться до тех пор, пока не выйдет нечто более совершенное.",
                tocTitle: "Содержание раздела",
                tocWhatAreLlms: "Что такое языковые модели и их история",
                tocTransformerArchitecture: "Архитектура Transformer",
                tocModelParameters: "Параметры моделей",
                tocQuantization: "Квантование",
                tocTokenization: "Токенизация",
                tocContext: "Контекст",
                tocFlashAttention: "Flash Attention",
                tocGgufFormat: "Формат GGUF",
                tocLlamaCpp: "Llama.CPP",
                whatAreLlmsTitle: "Что такое языковые модели и их история",
                whatAreLlmsText1: "Языковые модели (Large Language Models, LLM) представляют собой искусственные нейронные сети, обученные на огромных объемах текстовых данных для распознавания сложных паттернов, контекстных отношений и лингвистических нюансов в человеческом языке. Эти модели способны предсказывать вероятность следующего токена (слова или его части) в последовательности на основе предыдущего контекста.",
                whatAreLlmsText2: "История языковых моделей началась задолго до современных LLM. Еще в 1977 году был впервые применен алгоритм beam search для распознавания речи, который до сих пор используется как окончательный слой принятия решений в NLP-системах. Первоначально языковые модели были относительно простыми n-граммными моделями, где вероятность каждого слова зависела только от n-1 предыдущих слов. Однако с развитием технологий и увеличением вычислительных мощностей появились более сложные архитектуры.",
                whatAreLlmsText3: "Современные LLM, такие как GPT, Llama, Mistral и другие, представляют собой гигантские нейронные сети с миллиардами параметров. Обучение LLM требует колоссальных вычислительных ресурсов. Для обучения крупномасштабных моделей, таких как Llama или GPT, часто необходимо использовать десятки тысяч процессорных единиц. Большинство LLM обучаются на интернет-данных, включая Википедию, книги и другие текстовые источники, преобразованные в формат простого текста.",
                transformerArchitectureTitle: "Архитектура Transformer",
                transformerArchitectureText1: "Современные LLM, как правило, используют decoder-only Transformer архитектуру и механизмы self-attention. Transformer, представленный в 2017 году в статье \"Attention is All You Need\", кардинально изменил подход к обработке естественного языка, заменив рекуррентные и сверточные нейронные сети на механизмы внимания.",
                attentionPaperScreenshotAlt: "Статья Attention Is All You Need",
                attentionPaperCaption: "Статья \"Attention Is All You Need\"",
                transformerArchitectureText2: "Процесс работы LLM можно описать следующим образом:",
                transformerStep1: "Токенизация: Входной текст разбивается на токены с помощью токенизатора. Токены могут представлять собой целые слова, части слов или даже отдельные символы, в зависимости от используемого алгоритма токенизации.",
                transformerStep2: "Эмбеддинги: Каждому токену присваивается числовой вектор, который кодирует его семантическое значение. Этот вектор является отправной точкой для понимания модели. В некоторых архитектурах векторы также содержат информацию о позиции токена в последовательности, в других — эта информация добавляется позже.",
                transformerStep3: "Decoder Layers: Токены проходят через серию идентичных декодерных слоев (количество может варьироваться от 24 до 80+ в зависимости от размера модели).",
                transformerStep4: "Output Generation: После прохождения всех слоев модель использует функцию softmax для преобразования оценок, указывающих, насколько модель считает каждый токен вероятным, в вероятности. Эти вероятности показывают, насколько вероятен каждый токен как следующее слово в выходной последовательности.",
                modelParametersTitle: "Параметры моделей",
                modelParametersText1: "Параметры языковой модели - это численные значения, определяющие поведение нейронной сети. Чем больше параметров имеет модель, тем сложнее зависимости она может улавливать, однако для её запуска потребуется больше ресурсов. Размер модели обычно выражается в миллиардах параметров (7B, 13B, 70B и т.д.). Для оценки требований к памяти можно использовать простое правило: модель в формате FP16 занимает примерно 2 гигабайта памяти на каждый миллиард параметров. Например:",
                modelSizeExample1: "Модель 7B в FP16 занимает около 14 ГБ",
                modelSizeExample2: "Модель 13B в FP16 занимает около 26 ГБ",
                modelSizeExample3: "Модель 70B в FP16 занимает около 140 ГБ",
                modelParametersText2: "Однако для фактического запуска модели требуется немного больше памяти, чем размер самой модели, поскольку необходимо хранить данные текущих вычислений и обработанный контекст. Например, теоретический запуск модели 7B потребует минимум 16 ГБ видеопамяти (в реальности почти 20 ГБ). Это очень много для такого количества параметров, и мы можем оптимизировать вес модели благодаря такой технологии как квантование.",
                quantizationTitle: "Квантование",
                quantizationText1: "Квантование - это процесс сжатия языковой модели, который позволяет снизить точность представления весов модели с 16 бит до 8, 4, 3 или даже 2 бит, сохраняя при этом основной функционал и качество генерации. Это критически важно для запуска моделей на обычных домашних компьютерах с ограниченными ресурсами видеопамяти.",
                quantizationBenefit1: "Результаты при генерации токенов моделью, квантованной в 8 бит, практически не имеют отличий от 16-битной версии",
                quantizationBenefit2: "Большие модели (70B) лучше переносят квантование даже в малые битности (~3 бита), тогда как на малых моделях (7B) заметная деградация может проявиться уже при 4 битах",
                quantizationBenefit3: "Квантованные (особенно в малую битность) модели малопригодны для обучения",
                quantizationBenefit4: "Сильное ужатие (ниже 3 бит) приведет к поломке и деградации модели, вы это сразу почувствуете.",
                quantizationText2: "Для понимания квантования необходимо знать структуру языковой модели. Каждый слой модели состоит из двух основных типов тензоров:",
                tensorType1: "Тензоры внимания (attn) - отвечают за механизм self-attention, определяющий, насколько качественной будет квантованная модель",
                tensorType2: "Тензоры полносвязной сети (ffn) - отвечают за непосредственную обработку информации после механизма внимания",
                quantizationText3: "В формате GGUF квантование обозначается как Qx_K_S/M/L, где:",
                ggufFormat1: "Qx - основной уровень квантования для тензоров ffn (полносвязной сети), где x — число от 1 до 8",
                ggufFormat2: "K - указывает на версию формата квантования",
                ggufFormat3: "S/M/L - определяет, насколько выше будут квантованы важные тензоры внимания (attn)",
                ggufLevels1: "S (Small) - важные тензоры внимания квантуются на 1 шаг выше (Qx+1)",
                ggufLevels2: "M (Medium) - важные тензоры внимания квантуются на 2 шага выше (Qx+2)",
                ggufLevels3: "L (Large) - важные тензоры внимания квантуются на 3 шага выше (Qx+3)",
                quantizationExample: "К примеру, в Q4_K_M:",
                q4kmExample1: "Основной уровень квантования — 4 бита (Q4)",
                q4kmExample2: "Важные тензоры внимания квантуются на 2 шага выше — до 6 бит (Q6)",
                quantizationTypesTitle: "Типы квантования и их особенности:",
                staticQuantizationTitle: "Статическое квантование (Qx_K_S/M/L)",
                staticQuantizationText: "Это наиболее распространенный тип квантования. Важно понимать, что разные создатели квантов могут иметь разные интерпретации \"важных тензоров\". Например, некоторые квантуют ffn_up и ffn_gate на 1 шаг ниже, чем основной уровень, считая это оптимальным. Это приводит к ситуации, когда кванты с одинаковым названием (например, Q4_K_M) от разных создателей могут вести себя по-разному.",
                iqQuantizationTitle: "IQ кванты",
                iqQuantizationText1: "IQ кванты используют матрицу важности (imatrix), которая создается на основе типичного использования модели. Процесс создания imatrix включает сбор типичных примеров использования модели (тексты, программирование, факты и т.д.) и анализ, какие части модели активируются при их обработке.",
                iqQuantizationText2: "Преимущество IQ квантования:",
                iqBenefit1: "Уменьшение размера модели при сохранении качества",
                iqBenefit2: "Более эффективное распределение битности между тензорами",
                dynamicQuantizationTitle: "Динамическое квантование",
                dynamicQuantizationText1: "UD (Unsloth Dynamic 2.0)",
                udQuantizationText: "Это передовая технология квантования, которая динамически определяет, какие части модели требуют более высокой точности. Например, UD-Q2_K_XL (2-битное квантование) может давать качество, близкое к оригиналу, теряя всего несколько процентов на примере DeepSeek R1, будучи в 3.3 раза меньше по размеру (212 ГБ против 700 ГБ).",
                dynamicQuantizationText2: "R4",
                r4QuantizationText: "R4 - это state-of-the-art квантование от ikawrakow, позволяющее создать самый маленький квант для крупных моделей. Например, R4 позволяет создать квант DeepSeek R1 размером всего 130 ГБ, который может работать на домашних ПК с 128 ГБ RAM и одной GPU.",
                dynamicQuantizationText3: "iqN_rt (Trellis квантование)",
                trellisQuantizationText: "Это новейший тип квантования, аналогичный QTIP, используемому в exl3. Trellis квантование находит оптимальные параметры для каждого блока, позволяет лучше сохранить свойства оригинальной модели при минимальном количестве бит на вес (bpw).",
                tokenizationTitle: "Токенизация",
                tokenizationText1: "Токенизация - это фундаментальный процесс, который включает разделение заданного текста на отдельные единицы, называемые токенами. Токены могут представлять собой целые слова, части слов или даже отдельные символы, в зависимости от уровня детализации, необходимого для конкретной задачи. Причина необходимости токенизации достаточно проста: машины не понимают слов в их естественной форме, поэтому необходимо преобразовать слова в численные представления. Однако разделение текста по алфавитным буквам слишком неэффективно (потребуется в среднем 4 токена на одно слово), а разделение по словам тоже неоптимально (потребуется словарь размером около 171 476 слов только для английского языка). Поэтому создается небольшая модель токенизатора, обученная на большом объеме текста, чтобы обеспечить наиболее эффективную токенизацию с минимальным размером словаря. Например, токенизатор BPE (Byte Pair Encoding) или его варианты, используемые в современных моделях, создают словарь фиксированного размера (обычно 32 000-50 000 токенов), который может эффективно кодировать как целые слова, так и их части.",
                tokenizationText2: "Важно отметить, что разные языки имеют разную эффективность токенизации. Английский язык (и латиница в целом) потребляет наименьшее количество токенов (в среднем 1-2 на слово), тогда как кириллица может потребовать вдвое больше токенов на тот же объем символов. Эмодзи и специальные символы могут потребовать нескольких токенов на один символ.",
                contextTitle: "Контекст",
                contextText1: "Контекст - это объем текста, который модель обрабатывает за один раз. Он включает в себя историю чата, системный промт, карточки персонажей, авторские заметки, лорбуки и другие элементы, которые формируют основу для генерации ответа. Модель \"помнит\" и учитывает только то, что находится в пределах контекста. Базовый контекст в новых моделях составляет 4096 и более токенов, в более ранних моделях было 2048 или меньше. Это означает, что если ваши сообщения в чате, упоминание важного события или определение какого-то термина выходит за пределы этого контекста, модель перестанет помнить об этом и будет интерпретировать запрос на основе своих общих знаний.",
                contextText2: "Проблема ограниченного контекста решается несколькими способами:",
                contextSolution1: "Суммаризацией (созданием краткого содержания предыдущего контекста)",
                contextSolution2: "Использованием лорбуков (специальных хранилищ знаний с информацией о мире и персонажах)",
                contextSolution3: "Векторными базами данных для поиска релевантной информации",
                contextSolution4: "Простым увеличением контекста",
                contextText3: "Контекст можно увеличить через настройки LLM в Soul of Waifu, однако не забудьте перезагрузить локальную языковую модель, если на момент изменения значения контекстного окна она была запущена.",
                contextText4: "Сильное увеличение контекста (более чем в 2-3 раза относительно исходного для модели) может привести к ухудшению качества ответов. Некоторые модели изначально обучаются с использованием подобных параметров, что позволяет увеличивать контекст до огромных величин (32K, 64K, 100K и более) без существенного ухудшения качества.",
                flashAttentionTitle: "Flash Attention",
                flashAttentionText1: "Flash Attention - это оптимизация, разработанная для ускорения и снижения потребления памяти механизмов внимания в трансформерах. Эта технология особенно важна при работе с длинными контекстами, так как вычислительная сложность стандартного механизма внимания растет квадратично с увеличением длины последовательности. Стандартный механизм внимания требует O(n²) памяти и вычислений для последовательности длиной n, что становится узким местом при работе с длинными контекстами (8K, 32K, 128K токенов). Flash Attention решает эту проблему, оптимизируя вычисления и уменьшая потребление памяти. Включение Flash Attention экономит 20-30% VRAM для длинных контекстов и особенно эффективно при использовании llama.cpp, которая как раз и используется в нашей программе. Эта оптимизация уменьшает влияние контекстного окна на память, причем эффект более заметен при большем количестве контекста. В сочетании с квантованием контекста до 4-бит можно дополнительно снизить нагрузку на память. Flash Attention работает за счет более эффективной организации вычислений и использования специализированных ядер для GPU, что позволяет уменьшить количество обращений к памяти и оптимизировать использование вычислительных ресурсов.",
                ggufFormatTitle: "Формат GGUF",
                ggufFormatText1: "GGUF (GPT-Generated Unified Format) - это формат файлов для хранения квантованных языковых моделей, разработанный специально для использования с llama.cpp. Этот формат заменяет более старый формат GGML и предоставляет ряд улучшений и дополнительных возможностей:",
                ggufFeature1: "Все данные хранятся в одном файле (в отличие от других форматов, где модель распространяется папкой с множеством файлов)",
                ggufFeature2: "Поддерживает различные уровни квантования (от 2.5 до 8 бит)",
                ggufFeature3: "Включает метаданные, такие как архитектура модели, токенизатор",
                ggufFeature4: "Позволяет запускать модели на CPU, GPU или гибридно",
                ggufFeature5: "Поддерживает матрицу важности (Imatrix) для более эффективного квантования",
                ggufFormatText2: "Имена файлов GGUF следуют определенному шаблону: \"имя_модели-битность_формат.расширение\". Например, \"gemma-3-9b-q4_K_M.gguf\".",
                llamaCppTitle: "Llama.CPP",
                llamaCppText1: "llama.cpp - это один из самых популярных программ для запуска языковых моделей, разработанный Georgi Gerganov. Этот проект представляет собой высокопроизводительную реализацию языковых моделей на языке C/C++, оптимизированную для запуска на различных устройствах, включая CPU, GPU и даже мобильные платформы.",
                llamaCppText2: "Основные особенности llama.cpp:",
                llamaCppFeature1: "Кроссплатформенность: Работает на Windows, Linux, macOS, включая поддержку Apple Silicon через Metal.",
                llamaCppFeature2: "Гибкость в использовании ресурсов: Позволяет запускать модели как на CPU, так и на GPU, а также в гибридном режиме, где часть слоев обрабатывается на GPU, а часть — на CPU.",
                llamaCppFeature3: "Поддержка GGUF: Является основным движком для работы с моделями в формате GGUF.",
                llamaCppFeature4: "Offloading слоев: Одна из ключевых функций - возможность выбора количества слоев, которые будут загружены в видеопамять (VRAM). Это реализуется через настройку GPU-слоев в программе. Например, выставленное значение в 30 слоёв загрузит первые 30 слоев модели в VRAM, остальные будут обрабатываться вашим процессором.",
                llamaCppFeature5: "Поддержка различных оптимизаций: Например, Flash Attention для уменьшения потребления памяти при длинных контекстах.",
                llamaCppFeature6: "Производительность: Даже при запуске на CPU llama.cpp демонстрирует впечатляющую производительность благодаря оптимизации для современных архитектур (AVX2, AVX-512).",
                llamaCppText3: "И как раз Soul of Waifu использует llama.cpp для запуска локальных языковых моделей внутри себя. Надеюсь, вы узнали много нового о языковых моделях, а в следующем разделе мы поговорим о персонах - инструменте, которые позволят вам представить себя персонажу.",
                paginationPrev: "Аватары персонажей",
                paginationNext: "Персона",
                copyrightText: "© 2025 Soul of Waifu. Разработал jofizcd."
            },
            en: {
                pageTitle: "All About LLMs - Soul of Waifu Documentation",
                githubTitle: "GitHub",
                discordTitle: "Discord",
                sidebarTitle: "Table of Contents",
                sidebarIntroduction: "Introduction",
                sidebarInstallation: "Installation",
                sidebarQuickStart: "Quick Start",
                sidebarCharacterCreation: "Character Creation",
                sidebarLlmSetup: "LLM Setup",
                sidebarTtsStt: "Text-to-Speech",
                sidebarAvatars: "Character Avatars",
                sidebarCourseTitle: "Introduction to AI RP",
                sidebarCourseLlm: "All About LLMs",
                sidebarCoursePersona: "Persona",
                sidebarCourseSystemPrompt: "System Prompt",
                sidebarCourseLorebooks: "Lorebooks",
                sidebarCourseSmartMemory: "Smart Memory",
                sidebarCourseSoulSystem: "Soul of Waifu System",
                sidebarDevelopmentTitle: "Program Support",
                sidebarCommunity: "Community",
                sidebarSupport: "Support the Author",
                docTitle: "All About LLMs",
                docIntro: "Hi everyone, now we will learn some things about large and not-so-large language models, after which you will know their backstory, as well as why the Transformer technology is something we should worship until something more perfect comes along.",
                tocTitle: "Section Contents",
                tocWhatAreLlms: "What are language models and their history",
                tocTransformerArchitecture: "Transformer Architecture",
                tocModelParameters: "Model Parameters",
                tocQuantization: "Quantization",
                tocTokenization: "Tokenization",
                tocContext: "Context",
                tocFlashAttention: "Flash Attention",
                tocGgufFormat: "GGUF Format",
                tocLlamaCpp: "Llama.CPP",
                whatAreLlmsTitle: "What are language models and their history",
                whatAreLlmsText1: "Language models (Large Language Models, LLMs) are artificial neural networks trained on huge volumes of text data to recognize complex patterns, contextual relationships and linguistic nuances in human language. These models can predict the probability of the next token (word or its part) in a sequence based on the previous context.",
                whatAreLlmsText2: "The history of language models began long before modern LLMs. As early as 1977, the beam search algorithm was first applied for speech recognition, which is still used as the final decision-making layer in NLP systems. Initially, language models were relatively simple n-gram models, where the probability of each word depended only on the previous n-1 words. However, with the development of technology and increased computing power, more complex architectures emerged.",
                whatAreLlmsText3: "Modern LLMs, such as GPT, Llama, Mistral and others, are giant neural networks with billions of parameters. Training LLMs requires enormous computing resources. For training large-scale models, such as Llama or GPT, it is often necessary to use tens of thousands of processing units. Most LLMs are trained on internet data, including Wikipedia, books and other text sources converted to plain text format.",
                transformerArchitectureTitle: "Transformer Architecture",
                transformerArchitectureText1: "Modern LLMs generally use decoder-only Transformer architecture and self-attention mechanisms. Transformer, introduced in 2017 in the paper \"Attention is All You Need\", radically changed the approach to natural language processing, replacing recurrent and convolutional neural networks with attention mechanisms.",
                attentionPaperScreenshotAlt: "Attention Is All You Need Paper",
                attentionPaperCaption: "\"Attention Is All You Need\" Paper",
                transformerArchitectureText2: "The LLM operation process can be described as follows:",
                transformerStep1: "Tokenization: The input text is broken down into tokens using a tokenizer. Tokens can represent whole words, parts of words, or even individual characters, depending on the tokenization algorithm used.",
                transformerStep2: "Embeddings: Each token is assigned a numerical vector that encodes its semantic meaning. This vector is the starting point for model understanding. In some architectures, vectors also contain information about the token's position in the sequence, in others - this information is added later.",
                transformerStep3: "Decoder Layers: Tokens pass through a series of identical decoder layers (the number can vary from 24 to 80+ depending on the model size).",
                transformerStep4: "Output Generation: After passing through all layers, the model uses the softmax function to transform scores indicating how probable the model considers each token into probabilities. These probabilities show how likely each token is as the next word in the output sequence.",
                modelParametersTitle: "Model Parameters",
                modelParametersText1: "Language model parameters are numerical values that determine the behavior of the neural network. The more parameters a model has, the more complex dependencies it can capture, but more resources will be required to run it. Model size is usually expressed in billions of parameters (7B, 13B, 70B, etc.). For estimating memory requirements, a simple rule can be used: a model in FP16 format takes approximately 2 gigabytes of memory per billion parameters. For example:",
                modelSizeExample1: "7B model in FP16 takes about 14 GB",
                modelSizeExample2: "13B model in FP16 takes about 26 GB",
                modelSizeExample3: "70B model in FP16 takes about 140 GB",
                modelParametersText2: "However, slightly more memory is required to actually run the model than the size of the model itself, since it is necessary to store current computation data and processed context. For example, theoretically running a 7B model will require at least 16 GB of video memory (in reality, almost 20 GB). This is a lot for such a number of parameters, and we can optimize the model weight thanks to such technology as quantization.",
                quantizationTitle: "Quantization",
                quantizationText1: "Quantization is a process of compressing a language model that allows reducing the accuracy of representing model weights from 16 bits to 8, 4, 3, or even 2 bits, while preserving the main functionality and generation quality. This is critically important for running models on ordinary home computers with limited video memory resources.",
                quantizationBenefit1: "Results when generating tokens with an 8-bit quantized model practically have no difference from the 16-bit version",
                quantizationBenefit2: "Large models (70B) better tolerate quantization even to small bit depths (~3 bits), while noticeable degradation may appear on small models (7B) already at 4 bits",
                quantizationBenefit3: "Quantized models (especially in small bit depths) are poorly suited for training",
                quantizationBenefit4: "Strong compression (below 3 bits) will lead to model breakdown and degradation, you will feel this immediately.",
                quantizationText2: "To understand quantization, it is necessary to know the structure of the language model. Each model layer consists of two main types of tensors:",
                tensorType1: "Attention tensors (attn) - responsible for the self-attention mechanism, determining how high-quality the quantized model will be",
                tensorType2: "Fully connected network tensors (ffn) - responsible for direct information processing after the attention mechanism",
                quantizationText3: "In GGUF format, quantization is denoted as Qx_K_S/M/L, where:",
                ggufFormat1: "Qx - the main quantization level for ffn tensors (fully connected network), where x is a number from 1 to 8",
                ggufFormat2: "K - indicates the quantization format version",
                ggufFormat3: "S/M/L - determines how much higher the important attention tensors will be quantized",
                ggufLevels1: "S (Small) - important attention tensors are quantized 1 step higher (Qx+1)",
                ggufLevels2: "M (Medium) - important attention tensors are quantized 2 steps higher (Qx+2)",
                ggufLevels3: "L (Large) - important attention tensors are quantized 3 steps higher (Qx+3)",
                quantizationExample: "For example, in Q4_K_M:",
                q4kmExample1: "Main quantization level - 4 bits (Q4)",
                q4kmExample2: "Important attention tensors are quantized 2 steps higher - to 6 bits (Q6)",
                quantizationTypesTitle: "Types of Quantization and Their Features:",
                staticQuantizationTitle: "Static Quantization (Qx_K_S/M/L)",
                staticQuantizationText: "This is the most common type of quantization. It is important to understand that different quant creators may have different interpretations of \"important tensors\". For example, some quantize ffn_up and ffn_gate 1 step lower than the main level, considering this optimal. This leads to a situation where quants with the same name (for example, Q4_K_M) from different creators may behave differently.",
                iqQuantizationTitle: "IQ Quants",
                iqQuantizationText1: "IQ quants use an importance matrix (imatrix) that is created based on typical model usage. The imatrix creation process includes collecting typical examples of model usage (texts, programming, facts, etc.) and analyzing which parts of the model are activated during their processing.",
                iqQuantizationText2: "Advantage of IQ quantization:",
                iqBenefit1: "Reducing model size while maintaining quality",
                iqBenefit2: "More efficient bit allocation between tensors",
                dynamicQuantizationTitle: "Dynamic Quantization",
                dynamicQuantizationText1: "UD (Unsloth Dynamic 2.0)",
                udQuantizationText: "This is a cutting-edge quantization technology that dynamically determines which parts of the model require higher accuracy. For example, UD-Q2_K_XL (2-bit quantization) can produce quality close to the original, losing only a few percent on the DeepSeek R1 example, being 3.3 times smaller in size (212 GB vs 700 GB).",
                dynamicQuantizationText2: "R4",
                r4QuantizationText: "R4 is state-of-the-art quantization from ikawrakow that allows creating the smallest quant for large models. For example, R4 allows creating a DeepSeek R1 quant of only 130 GB that can run on home PCs with 128 GB RAM and one GPU.",
                dynamicQuantizationText3: "iqN_rt (Trellis Quantization)",
                trellisQuantizationText: "This is a cutting-edge quantization type, similar to QTIP used in exl3. Trellis quantization finds optimal parameters for each block, allowing better preservation of the original model properties with minimal bits per weight (bpw).",
                tokenizationTitle: "Tokenization",
                tokenizationText1: "Tokenization is a fundamental process that involves dividing a given text into individual units called tokens. Tokens can represent whole words, parts of words, or even individual characters, depending on the level of detail required for a specific task. The reason for tokenization is quite simple: machines do not understand words in their natural form, so it is necessary to convert words into numerical representations. However, dividing text by alphabetic letters is too inefficient (an average of 4 tokens per word will be required), and dividing by words is also suboptimal (a dictionary of about 171,476 words would be required for English alone). Therefore, a small tokenizer model is created, trained on a large volume of text, to provide the most efficient tokenization with a minimal dictionary size. For example, the BPE (Byte Pair Encoding) tokenizer or its variants used in modern models create a fixed-size dictionary (usually 32,000-50,000 tokens) that can efficiently encode both whole words and their parts.",
                tokenizationText2: "It is important to note that different languages have different tokenization efficiency. English (and Latin in general) consumes the least number of tokens (on average 1-2 per word), while Cyrillic may require twice as many tokens for the same volume of characters. Emojis and special characters may require several tokens per character.",
                contextTitle: "Context",
                contextText1: "Context is the volume of text that the model processes at one time. It includes chat history, system prompt, character cards, author's notes, lorebooks and other elements that form the basis for generating a response. The model \"remembers\" and takes into account only what is within the context. The basic context in new models is 4096 or more tokens, in earlier models it was 2048 or less. This means that if your chat messages, mention of an important event, or definition of some term goes beyond this context, the model will stop remembering it and will interpret the request based on its general knowledge.",
                contextText2: "The problem of limited context is solved in several ways:",
                contextSolution1: "Summarization (creating a brief summary of the previous context)",
                contextSolution2: "Using lorebooks (special knowledge repositories with information about the world and characters)",
                contextSolution3: "Vector databases for searching relevant information",
                contextSolution4: "Simply increasing the context",
                contextText3: "Context can be increased through LLM settings in Soul of Waifu, but remember to reload the local language model if it was running at the time of changing the context window value.",
                contextText4: "Significant context increase (more than 2-3 times relative to the original for the model) may lead to deterioration in response quality. Some models are initially trained using such parameters, which allows increasing the context to huge values (32K, 64K, 100K and more) without significant quality deterioration.",
                flashAttentionTitle: "Flash Attention",
                flashAttentionText1: "Flash Attention is an optimization developed to speed up and reduce memory consumption of attention mechanisms in transformers. This technology is especially important when working with long contexts, as the computational complexity of the standard attention mechanism grows quadratically with increasing sequence length. The standard attention mechanism requires O(n²) memory and computations for a sequence of length n, which becomes a bottleneck when working with long contexts (8K, 32K, 128K tokens). Flash Attention solves this problem by optimizing computations and reducing memory consumption. Enabling Flash Attention saves 20-30% VRAM for long contexts and is especially effective when using llama.cpp, which is exactly what is used in our program. This optimization reduces the impact of the context window on memory, with the effect being more noticeable with more context. In combination with 4-bit context quantization, the memory load can be further reduced. Flash Attention works by more efficiently organizing computations and using specialized kernels for GPU, which allows reducing memory accesses and optimizing the use of computing resources.",
                ggufFormatTitle: "GGUF Format",
                ggufFormatText1: "GGUF (GPT-Generated Unified Format) is a file format for storing quantized language models, developed specifically for use with llama.cpp. This format replaces the older GGML format and provides a number of improvements and additional features:",
                ggufFeature1: "All data is stored in one file (unlike other formats where the model is distributed as a folder with multiple files)",
                ggufFeature2: "Supports various quantization levels (from 2.5 to 8 bits)",
                ggufFeature3: "Includes metadata such as model architecture, tokenizer",
                ggufFeature4: "Allows running models on CPU, GPU or hybridly",
                ggufFeature5: "Supports importance matrix (Imatrix) for more efficient quantization",
                ggufFormatText2: "GGUF file names follow a specific pattern: \"model_name-bit_depth_format.extension\". For example, \"gemma-3-9b-q4_K_M.gguf\".",
                llamaCppTitle: "Llama.CPP",
                llamaCppText1: "llama.cpp is one of the most popular programs for running language models, developed by Georgi Gerganov. This project is a high-performance implementation of language models in C/C++, optimized for running on various devices, including CPU, GPU and even mobile platforms.",
                llamaCppText2: "Main features of llama.cpp:",
                llamaCppFeature1: "Cross-platform: Works on Windows, Linux, macOS, including support for Apple Silicon via Metal.",
                llamaCppFeature2: "Resource usage flexibility: Allows running models on both CPU and GPU, as well as in hybrid mode, where part of the layers are processed on GPU and part on CPU.",
                llamaCppFeature3: "GGUF support: Is the main engine for working with models in GGUF format.",
                llamaCppFeature4: "Layer offloading: One of the key features - the ability to select the number of layers that will be loaded into video memory (VRAM). This is implemented through the GPU layers setting in the program. For example, a value of 30 layers will load the first 30 layers of the model into VRAM, the rest will be processed by your processor.",
                llamaCppFeature5: "Support for various optimizations: For example, Flash Attention to reduce memory consumption with long contexts.",
                llamaCppFeature6: "Performance: Even when running on CPU, llama.cpp demonstrates impressive performance thanks to optimization for modern architectures (AVX2, AVX-512).",
                llamaCppText3: "And Soul of Waifu uses llama.cpp to run local language models inside itself. I hope you learned a lot about language models, and in the next section we will talk about personas - a tool that will allow you to present yourself to the character.",
                paginationPrev: "Character Avatars",
                paginationNext: "Persona",
                copyrightText: "© 2025 Soul of Waifu. Developed by jofizcd."
            }
        };
        function saveSettings(settings) {
            try {
                localStorage.setItem('soulofwaifu_settings', JSON.stringify(settings));
            } catch (e) {
                console.warn("Не удалось сохранить настройки в localStorage:", e);
            }
        }
        function loadSettings() {
            try {
                const settingsStr = localStorage.getItem('soulofwaifu_settings');
                if (settingsStr) {
                    return JSON.parse(settingsStr);
                }
            } catch (e) {
                console.warn("Не удалось загрузить настройки из localStorage:", e);
            }
            return { theme: 'dark', language: 'ru' };
        }
        function applySettings(settings) {
            if (settings.theme === 'light') {
                document.body.classList.add('light-theme');
                const logoDark = document.querySelector('.logo-dark');
                const logoLight = document.querySelector('.logo-light');
                if (logoDark) logoDark.style.display = 'none';
                if (logoLight) logoLight.style.display = 'block';
            } else {
                document.body.classList.remove('light-theme');
                const logoDark = document.querySelector('.logo-dark');
                const logoLight = document.querySelector('.logo-light');
                if (logoDark) logoDark.style.display = 'block';
                if (logoLight) logoLight.style.display = 'none';
            }
            updateLanguage(settings.language);
            if (document.getElementById('languageSelector')) {
                document.getElementById('languageSelector').value = settings.language;
            }
        }
        document.addEventListener('DOMContentLoaded', function() {
            const themeToggle = document.getElementById('themeToggle');
            if (themeToggle) {
                themeToggle.addEventListener('click', function() {
                    document.body.classList.toggle('light-theme');
                    const logoDark = document.querySelector('.logo-dark');
                    const logoLight = document.querySelector('.logo-light');
                    if (document.body.classList.contains('light-theme')) {
                        if (logoDark) logoDark.style.display = 'none';
                        if (logoLight) logoLight.style.display = 'block';
                    } else {
                        if (logoDark) logoDark.style.display = 'block';
                        if (logoLight) logoLight.style.display = 'none';
                    }
                    const currentLang = document.getElementById('languageSelector') ? document.getElementById('languageSelector').value : 'ru';
                    const currentTheme = document.body.classList.contains('light-theme') ? 'light' : 'dark';
                    saveSettings({ theme: currentTheme, language: currentLang });
                });
            }
            const languageSelector = document.getElementById('languageSelector');
            if (languageSelector) {
                languageSelector.addEventListener('change', function() {
                    const selectedLang = this.value;
                    updateLanguage(selectedLang);
                    const currentTheme = document.body.classList.contains('light-theme') ? 'light' : 'dark';
                    saveSettings({ theme: currentTheme, language: selectedLang });
                });
            }
            const savedSettings = loadSettings();
            applySettings(savedSettings);
            const currentPagePath = window.location.pathname; 
            const sidebarLinks = document.querySelectorAll('.doc-sidebar a');
            sidebarLinks.forEach(link => {
                try {
                    const linkPath = new URL(link.href, window.location.origin).pathname;
                    link.classList.remove('active');
                    if (currentPagePath === linkPath || 
                        currentPagePath === linkPath.replace('/index.html', '/') ||
                        (currentPagePath.endsWith('/') && linkPath.endsWith('/index.html')) ||
                        (currentPagePath === '/' && linkPath.includes('overview.html'))) {
                        link.classList.add('active');
                    }
                } catch (e) {
                    if (link.getAttribute('href').startsWith('#')) {
                        const currentHash = window.location.hash;
                        if (link.getAttribute('href') === currentHash) {
                            link.classList.add('active');
                        }
                    }
                }
            });
        });
        function updateLanguage(lang) {
            const t = translations[lang];
            if (!t) return;
            document.querySelectorAll('[data-translate-key]').forEach(element => {
                const key = element.getAttribute('data-translate-key');
                if (t[key]) {
                    if (element.tagName === 'INPUT' || element.tagName === 'TEXTAREA') {
                        element.placeholder = t[key];
                    } else {
                        element.textContent = t[key];
                    }
                }
            });
            document.querySelectorAll('[data-translate-placeholder]').forEach(element => {
                const key = element.getAttribute('data-translate-placeholder');
                if (t[key]) {
                    element.placeholder = t[key];
                }
            });
            document.querySelectorAll('[data-translate-title]').forEach(element => {
                const key = element.getAttribute('data-translate-title');
                if (t[key]) {
                    element.title = t[key];
                }
            });
            document.querySelectorAll('[data-translate-alt]').forEach(element => {
                const key = element.getAttribute('data-translate-alt');
                if (t[key]) {
                    element.alt = t[key];
                }
            });
            const prevLink = document.querySelector('.pagination-prev');
            if (prevLink) {
                prevLink.innerHTML = `<i class="fas fa-arrow-left"></i> ${t.paginationPrev || 'Installation'}`;
            }
            const nextLink = document.querySelector('.pagination-next');
            if (nextLink) {
                nextLink.innerHTML = `${t.paginationNext || 'Character Creation'} <i class="fas fa-arrow-right"></i>`;
            }
            document.title = t.pageTitle || "All About LLMs - Soul of Waifu Documentation";
        }
    </script>
</body>
</html>
